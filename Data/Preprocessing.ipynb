{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded '../Data/raw_Dataset.csv' successfully.\n",
      "Starting text preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2653/2653 [00:00<00:00, 3209.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed.\n",
      "Removing duplicate texts based on 'cleaned_text' column...\n",
      "Removed 33 duplicate entries. Dataset size reduced from 2653 to 2620.\n",
      "Preprocessed and deduplicated dataset saved to '../Preprocessed_Manual_Labelled_Dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## -----------------------------------------------------------------------\n",
    "## FOR Raw_Manual_Labelled_Dataset\n",
    "## -----------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # Enable tqdm for pandas\n",
    "\n",
    "# Initialize the stemmer (replace with actual stemmer initialization)\n",
    "# For demonstration, I'll create a dummy stemmer\n",
    "class SinhalaStemmer:\n",
    "    def stem(self, word):\n",
    "        # Dummy stemmer: returns the word as is\n",
    "        return {'stem': word}\n",
    "\n",
    "# Define a basic English-to-Sinhala dictionary\n",
    "english_to_sinhala = {\n",
    "    \"mental\": \"මානසික\",\n",
    "    \"health\": \"සෞඛය\",\n",
    "    \"issue\": \"ගැටළුව\",\n",
    "    \"problem\": \"ගැටළුව\",\n",
    "    \"anxiety\": \"කංසාව\",\n",
    "    \"depression\": \"විශාදය\",\n",
    "    \"panic\": \"සන්ත්රාස\",\n",
    "    \"attack\": \"වාර්තාව\",\n",
    "    \"panic attack\": \"සන්ත්රාස කාංසාව\",\n",
    "    \"panic attacks\": \"සන්ත්රාස කාංසාව\",\n",
    "    \"overthinking\": \"ඕනාවට වඩා සිතීම\",\n",
    "    \"pulstating\": \"ස්පන්දනය\",\n",
    "    \"social\": \"සමාජ\",\n",
    "    \"social anxiety\": \"සමාජ කාංසාව\",\n",
    "    \"busy\": \"කාර්යබහුල\",\n",
    "    \"work\": \"වැඩ\",\n",
    "    \"stress\": \"තදබදය\",\n",
    "    \"worry\": \"සැලකිලිමත් වීම / වායි\",\n",
    "    \"fear\": \"භීතිය\",\n",
    "    \"insomnia\": \"නිදාගත නොහැකියාව\",\n",
    "    \"sadness\": \"දුක්\",\n",
    "    \"lonely\": \"පාලුවෙන්\",\n",
    "    \"overwhelmed\": \"ඇති ආකාරයෙන් පීඩනයෙන්\",\n",
    "    \"tired\": \"දුස්ස්වීම\",\n",
    "    \"nervous\": \"සන්සුන් නොවන\",\n",
    "    \"exhausted\": \"වියළී ගිය\",\n",
    "    \"helpless\": \"අසරණ\",\n",
    "    \"restless\": \"නොසන්සුන්\",\n",
    "    \"hopeless\": \"බලාපොරොත්තු නැති\",\n",
    "    \"frustrated\": \"හුස්ම ගන්න බැරි මට්ටමින්\",\n",
    "    \"pressure\": \"පීඩනය\",\n",
    "    \"overthink\": \"ඕනෑවට වඩා සිතන්න\",\n",
    "    \"symptoms\": \"ලක්ෂණ\",\n",
    "    \"burden\": \"බර\",\n",
    "    \"nervousness\": \"ඉඟි ගැන්වීම\",\n",
    "    \"worried\": \"කනස්සල්ලට පත්\",\n",
    "    \"sensitive\": \"සැලකිලිමත්\",\n",
    "    \"broken\": \"බිඳුණු\",\n",
    "    \"nauseous\": \"නොසන්සුන්\",\n",
    "    \"sleep\": \"නිද्रा\",\n",
    "    \"lethargic\": \"අක්‍රිය\",\n",
    "    \"disturbing\": \"ප්‍රයෝජනවත් නොවන\",\n",
    "    \"anxious\": \"කනස්සල්ලට පත්\",\n",
    "    \"calm\": \"සන්සුන්\",\n",
    "    \"disorder\": \"රෝග\",\n",
    "    \"strange\": \"අමුතු\",\n",
    "    \"worriedness\": \"කනස්සල්ල\",\n",
    "    \"worrying\": \"කනස්සලද\",\n",
    "    \"tears\": \"කඳුළු\",\n",
    "    \"chaos\": \"අව්‍යවස්ථාව\",\n",
    "    \"feelings\": \"සංවේදන\",\n",
    "    \"feeling\": \"සංවේදනය\",\n",
    "    \"decision\": \"තීරණය\",\n",
    "    \"decisions\": \"තීරණ\",\n",
    "    \"sorrow\": \"දුක\",\n",
    "    \"sorrows\": \"දුක්ක\",\n",
    "    \"soul\": \"ආත්මය\",\n",
    "    \"unsettled\": \"නොසන්සුන්\",\n",
    "    \"dizzy\": \"වටකුරු\",\n",
    "    \"social media\": \"සමාජ මාධ්‍ය\",\n",
    "    \"idk\": \"මට නොදැනේ\",\n",
    "    \"unclear\": \"අවැගහන\",\n",
    "    \"bad\": \"නරක\",\n",
    "    \"thoughts\": \"සිතැගි\",\n",
    "    \"restlessness\": \"අශාන්තිකතාව\",\n",
    "    \"scared\": \"බියගත්\",\n",
    "    \"severe\": \"මහා\",\n",
    "    \"mental\": \"මානසික\",\n",
    "    \"health\": \"සෞඛ්‍යය\",\n",
    "    \"pain\": \"වේදනාව\",\n",
    "    \"burden \": \"බර \",\n",
    "    \"worry\": \"කනස්සල්ල\",\n",
    "    \"restless\": \"අශාන්තික\",\n",
    "    \"naturally\": \"ස්වාභාවිකවම\",\n",
    "    \"despair\": \"අපේක්ෂාව\",\n",
    "    \"expectations\": \"අපේක්ෂා\",\n",
    "    \"reason\": \"හේතුව\",\n",
    "    \"survive\": \"ගැලවීම\",\n",
    "    \"compromise\": \"සන්සුන් වීම\",\n",
    "    \"upset\": \"කලබලකාරී\",\n",
    "    \"dizziness\": \"වටකුරුකම\",\n",
    "    \"tiredness\": \"කලබල\",\n",
    "    \"crying\": \"අඳුනෑම\",\n",
    "    \"chest\": \"පොටුව\",\n",
    "    \"pain\": \"වේදනාව\",\n",
    "    \"afraid\": \"බියගත්\",\n",
    "    \"sudden\": \"හදිසි\",\n",
    "    \"frantic\": \"ගැලවුණු\",\n",
    "    \"emotional\": \"සංවේදී\",\n",
    "    \"over\": \"අධික\",\n",
    "    \"prevention\": \"ආරක්ෂාව\",\n",
    "    \"nausea\": \"ඔක්කාරය\",\n",
    "    \"doubts\": \"සැක\",\n",
    "    \"illness\": \"ආබාධය\",\n",
    "    \"serious\": \"ගැඹුරු\",\n",
    "    \"condition\": \"තත්ත්වය\",\n",
    "    \"discouraged\": \"අත්හරින්න\",\n",
    "    \"losing \": \"අහිමිවීම \",\n",
    "    \"time\": \"කාලය\",\n",
    "    \"run\": \"දුවනවා\",\n",
    "    \"out\": \"පිටතට\",\n",
    "    \"older\": \"වයස්ගත\",\n",
    "    \"bother\": \"හිත\",\n",
    "    \"tight\": \"තද\",\n",
    "    \"chest\": \"පොටුව\",\n",
    "    \"weight\": \"බර\",\n",
    "    \"loss\": \"අහිමිවීම\",\n",
    "    \"excessive\": \"අධික\",\n",
    "    \"hurt\": \"ගොඩක්\",\n",
    "    \"stop\": \"නවත්වන්න\",\n",
    "    \"let\": \"අවසන් කරන්න\",\n",
    "    \"down\": \"පහලට\",\n",
    "    \"exhaustion\": \"ශාක්තිමත්\",\n",
    "    \"agonize\": \"වේදනාවට\",\n",
    "    \"cry\": \"කඳුළු වන්න\",\n",
    "    \"frustration\": \"අසාර්ථක බව\",\n",
    "    \"guilt\": \"අපරාධ බර\",\n",
    "    \"lonely\": \"තනිකඩ\",\n",
    "    \"trigger\": \"සක්‍රීය කරන්න\",\n",
    "    \"stutter\": \"විකාශනය\",\n",
    "    \"extremely\": \"අතිශය\",\n",
    "    \"future\": \"අනාගතය\",\n",
    "    \"terrible\": \"දරුණු\",\n",
    "    \"therapy\": \"සංවර්ධන\",\n",
    "    \"medicine\": \"ඖෂධය\",\n",
    "    \"medication\": \"ඖෂධ ප්‍රතිකාරය\",\n",
    "    \"abandonment\": \"අත්හැරීම\",\n",
    "    \"insecure\": \"අරක්ෂිත\",\n",
    "    \"shortness\": \"කෙටි බව\",\n",
    "    \"breath\": \"ආශ්වාසය\",\n",
    "    \"unbearable\": \"අසහනය\",\n",
    "    \"struggle\": \"සටන්\"\n",
    "}\n",
    "\n",
    "# Additional common words and Sinhala names to remove as stopwords\n",
    "additional_useless_words = [\n",
    "    \"මම\", \"මට\", \"මගේ\", \"මන්\", \"මං\", \"කරන්න\", \"කියලා\", \"එක\", \"වගේ\", \"ගොඩක්\", \"ඔහෙ\",\n",
    "    \"ඔයා\", \"අපි\", \"හරි\", \"මෙහෙම\", \"එහෙම\", \"අද\", \"ඔහු\", \"ඇය\", \"ඔවුන්\", \"අපේ\",\n",
    "    \"ඉතින්\", \"දැන්\", \"ඔය\", \"මේක\", \"හැබැයි\", \"කොහෙන්\", \"කොහොමද\", \"කොහෙද\", \"කොහෙත්\",\n",
    "    \"කියනවා\", \"කියන\", \"කරපු\", \"කල\", \"අලුත්\", \"ඇත්ත\", \"වඩා\", \"වගේම\", \"එහේ\", \"මෙහේ\",\n",
    "    \"තව\", \"ඔබ\", \"ඔබගේ\", \"නැති\", \"හෙට\", \"රැයට\", \"දවස්\", \"පිට\", \"වඩාත්\", \"ඕනෙ\",\n",
    "    \"ඔව්\", \"නැහැ\", \"ඒක\", \"මේක\", \"ඒකද\", \"නැද්ද\", \"එහෙනම්\", \"කවුරුත්\", \"කවුරුද\",\n",
    "    \"මොකක්ද\", \"මොනවද\", \"මොනාද\", \"යන්න\", \"දෙන්න\", \"හිත\", \"හිතලා\", \"හිතනවා\",\n",
    "    \"කය\", \"අනිත්\", \"කොහොමහරි\", \"මොකක් වුනත්\", \"වාගේ\", \"ඔනෑම\", \"වෙන්න\",\n",
    "    \"තිබෙනවා\", \"තිබුනා\", \"තිබුණා\", \"කරලා\", \"කරනවා\", \"කලහ\", \"දකින්න\", \"දැක්කා\",\n",
    "    \"දක්වයි\", \"තෝරාගත්\", \"ගන්න\", \"එබඳු\", \"කෙසේද\", \"වර්තමානයෙ\", \"සෑහෙන\", \n",
    "    \"එහෙයින්\", \"බොහෝවිට\", \"අනිවාර්යයෙන්\", \"තවම\", \"හෝද\", \"නැද්ද\",\n",
    "    \"හැඩකට\", \"කියවන්න\", \"ලැබෙයි\", \"දවස\", \"ඔබේ\", \"ඔබට\", \"එයා\", \"කෙනෙක්\",\n",
    "    \"කැමතිනම්\", \"කියල\", \"එත්\", \"එක්ක\", \"දෙයක්\", \"වීඩියෝව\", \"ඊමේල්\", \"දුරකථන\", \"හරහා\", \"නාලිකාව\", \n",
    "    \"මගෙ\", \"එකක්\", \"ඇයට\", \"ඇඩ්මින්\", \"බික්ශූන්\", \"ත්\", \"මටත්\", \"බන්\", \"උබ\", \"ඔයාගේ\"\n",
    "]\n",
    "\n",
    "# Extend original stopwords with additional common words and Sinhala names\n",
    "sinhala_stopwords = [\n",
    "    \"සහ\", \"සමග\", \"සමඟ\", \"අහා\", \"ආහ්\", \"ආ\", \"ඕහෝ\", \"අනේ\", \"අඳෝ\", \"අපොයි\", \"අපෝ\", \n",
    "    \"අයියෝ\", \"ආයි\", \"ඌයි\", \"චී\", \"චිහ්\", \"චික්\", \"හෝ\", \"දෝ\", \"මෙන්\", \"සේ\", \"වැනි\", \n",
    "    \"බඳු\", \"වන්\", \"අයුරු\", \"අයුරින්\", \"ලෙස\", \"වැඩි\", \"ශ්‍රී\", \"හා\", \"ය\", \"නිසා\", \n",
    "    \"නිසාවෙන්\", \"බවට\", \"බව\", \"බවෙන්\", \"නම්\", \"සිට\", \"දී\", \"මහා\", \"මහ\", \"පමණ\", \n",
    "    \"පමණින්\", \"වන\", \"විට\", \"විටින්\", \"මේ\", \"මෙලෙස\", \"මෙයින්\", \"ඇති\", \"ලෙස\", \"සිදු\", \n",
    "    \"වශයෙන්\", \"යන\", \"සඳහා\", \"මගින්\", \"ඉතා\", \"ඒ\", \"එම\", \"ද\", \"අතර\", \"විසින්\", \n",
    "    \"පිළිබඳව\", \"පිළිබඳ\", \"තුළ\", \"වෙත\", \"වෙතින්\", \"වෙතට\", \"වෙනුවෙන්\", \"වෙනුවට\", \n",
    "    \"වෙන\", \"ගැන\", \"නෑ\", \"අනුව\", \"පරිදි\", \"තෙක්\", \"මෙතෙක්\", \"මේතාක්\", \"තුරු\", \"තුරා\", \"තුරාවට\", \"තුලින්\", \"නමුත්\", \n",
    "    \"එනමුත්\", \"වස්\", \"මෙන්\", \"පරිදි\", \"සෑම\", \"ඔබ\", \"අවශ්ය\", \"ඔබගේ\", \"වුණා\", \n",
    "    \"කෙතරම්\", \"කෙසේ\", \"එක්\", \"සමග\", \"තවත්\"\n",
    "]\n",
    "sinhala_stopwords.extend(additional_useless_words)\n",
    "\n",
    "stemmer = SinhalaStemmer()\n",
    "\n",
    "def unicode_normalize(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F6FF\"  # Emoticons\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols & Pictographs Extended-A\n",
    "        u\"\\u200d\"                 # Zero Width Joiner\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def contains_repeated_phrases(text):\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - 3):\n",
    "        if words[i] == words[i+1] == words[i+2]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_repeated_phrases(text):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if i + 2 < len(words) and words[i] == words[i+1] == words[i+2]:\n",
    "            repeated_word = words[i]\n",
    "            filtered_words.append(repeated_word)\n",
    "            j = i + 3\n",
    "            while j < len(words) and words[j] == repeated_word:\n",
    "                j += 1\n",
    "            i = j\n",
    "        else:\n",
    "            filtered_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def is_english_word(word):\n",
    "    return re.match(r'^[A-Za-z]+$', word) is not None\n",
    "\n",
    "def translate_english_to_sinhala(text):\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    for w in words:\n",
    "        if is_english_word(w):\n",
    "            # Convert to lowercase to match dictionary keys\n",
    "            w_lower = w.lower()\n",
    "            if w_lower in english_to_sinhala:\n",
    "                translated_words.append(english_to_sinhala[w_lower])\n",
    "            else:\n",
    "                translated_words.append(w)  # Retain if not in dictionary\n",
    "        else:\n",
    "            translated_words.append(w)\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "def remove_short_tokens(tokens, min_length=2):\n",
    "    return [t for t in tokens if len(t) >= min_length]\n",
    "\n",
    "def sinhala_stem(word):\n",
    "    stem_result = stemmer.stem(word)\n",
    "    if isinstance(stem_result, dict) and 'stem' in stem_result:\n",
    "        return stem_result['stem']\n",
    "    return word\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [sinhala_stem(t) for t in tokens]\n",
    "\n",
    "def clean_sinhala_token(token):\n",
    "    token = token.replace('\\u200d', '')\n",
    "    return token.strip()\n",
    "\n",
    "def filter_non_sinhala_tokens(tokens):\n",
    "    sinhala_pattern = re.compile(r'^[\\u0D80-\\u0DFF]+$')  # Sinhala Unicode range\n",
    "    filtered = []\n",
    "    for t in tokens:\n",
    "        t = clean_sinhala_token(t)\n",
    "        if sinhala_pattern.match(t):\n",
    "            filtered.append(t)\n",
    "    return filtered\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Unicode normalization\n",
    "    text = unicode_normalize(text)\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = remove_urls(text)\n",
    "    \n",
    "    # Step 3: Remove mentions\n",
    "    text = remove_mentions(text)\n",
    "    \n",
    "    # Step 4: Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Step 5: Remove digits\n",
    "    text = remove_digits(text)\n",
    "    \n",
    "    # Step 6: Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    # Step 7: Convert to lowercase\n",
    "    text = to_lowercase(text)\n",
    "    \n",
    "    # Step 8: Remove extra spaces\n",
    "    text = remove_extra_spaces(text)\n",
    "\n",
    "    # Step 9: Remove repeated phrases if any\n",
    "    if contains_repeated_phrases(text):\n",
    "        text = remove_repeated_phrases(text)\n",
    "\n",
    "    # Step 10: Translate English words to Sinhala\n",
    "    text = translate_english_to_sinhala(text)\n",
    "\n",
    "    # Step 11: Tokenize the text\n",
    "    tokens = basic_tokenize(text)\n",
    "    \n",
    "    # Step 12: Remove stopwords\n",
    "    tokens = remove_stopwords(tokens, sinhala_stopwords)\n",
    "    \n",
    "    # Step 13: Remove short tokens\n",
    "    tokens = remove_short_tokens(tokens, min_length=2)\n",
    "    \n",
    "    # Step 14: Filter non-Sinhala tokens\n",
    "    tokens = filter_non_sinhala_tokens(tokens)\n",
    "    \n",
    "    # Step 15: Stem tokens\n",
    "    tokens = stem_tokens(tokens)\n",
    "    \n",
    "    # Final cleaned text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text, tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Loading the Dataset\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Define the input and output file paths\n",
    "    input_csv = \"../Data/raw_Dataset.csv\"\n",
    "    output_csv = \"../Preprocessed_Manual_Labelled_Dataset.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Load the raw, unlabelled dataset\n",
    "        df = pd.read_csv(input_csv)\n",
    "        print(f\"Loaded '{input_csv}' successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{input_csv}' not found. Please check the file path.\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading '{input_csv}': {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Check if 'raw_text' column exists\n",
    "    if 'raw_text' not in df.columns:\n",
    "        print(\"Error: 'raw_text' column not found in the input CSV.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Preprocessing the Text Data\n",
    "    # -------------------------------\n",
    "    \n",
    "    print(\"Starting text preprocessing...\")\n",
    "    df['cleaned_text'], df['cleaned_tokens'] = zip(*df['raw_text'].progress_apply(preprocess_text))\n",
    "    print(\"Text preprocessing completed.\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Removing Duplicate Texts\n",
    "    # -------------------------------\n",
    "    \n",
    "    print(\"Removing duplicate texts based on 'cleaned_text' column...\")\n",
    "    \n",
    "    # Before removing duplicates\n",
    "    initial_count = df.shape[0]\n",
    "    \n",
    "    # Drop duplicate 'cleaned_text' entries, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['cleaned_text'], keep='first').reset_index(drop=True)\n",
    "    \n",
    "    # After removing duplicates\n",
    "    final_count = df.shape[0]\n",
    "    duplicates_removed = initial_count - final_count\n",
    "    \n",
    "    print(f\"Removed {duplicates_removed} duplicate entries. Dataset size reduced from {initial_count} to {final_count}.\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Saving the Preprocessed Dataset\n",
    "    # -------------------------------\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Preprocessed and deduplicated dataset saved to '{output_csv}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving preprocessed dataset: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
